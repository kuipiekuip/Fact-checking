{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8610763,"sourceType":"datasetVersion","datasetId":5016780},{"sourceId":8623186,"sourceType":"datasetVersion","datasetId":5154768},{"sourceId":8653967,"sourceType":"datasetVersion","datasetId":5184040},{"sourceId":8673137,"sourceType":"datasetVersion","datasetId":5180262},{"sourceId":54188,"sourceType":"modelInstanceVersion","modelInstanceId":44487},{"sourceId":59792,"sourceType":"modelInstanceVersion","modelInstanceId":50015},{"sourceId":62313,"sourceType":"modelInstanceVersion","modelInstanceId":52040},{"sourceId":63766,"sourceType":"modelInstanceVersion","modelInstanceId":53178},{"sourceId":63878,"sourceType":"modelInstanceVersion","modelInstanceId":53272},{"sourceId":64669,"sourceType":"modelInstanceVersion","modelInstanceId":53933}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nimport json\nimport pickle\n\n# If there's a GPU available...\nif torch.cuda.is_available():\n\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \nwith open(\"/kaggle/input/quantemp/test_claims_quantemp.json\", 'rb') as f:\n    test_data = json.load(f)\n        \ntest_data[-1]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T12:19:11.098864Z","iopub.execute_input":"2024-06-13T12:19:11.099344Z","iopub.status.idle":"2024-06-13T12:19:11.175640Z","shell.execute_reply.started":"2024-06-13T12:19:11.099306Z","shell.execute_reply":"2024-06-13T12:19:11.174671Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'crawled_date': '2022-09-24',\n 'country_of_origin': 'india',\n 'label': 'False',\n 'url': 'https://www.indiatoday.in/fact-check/story/fact-check-woman-seen-with-rahul-gandhi-is-not-amulya-leona-student-who-chanted-pak-zindabad-2004338-2022-09-24',\n 'lang': 'en',\n 'claim': \"During the Bharat Jodo Yatra, Rahul Gandhi was seen with Amulya Leona Noronha, the woman who was arrested for saying 'Pakistan Zindabad' during an anti-CAA rally in 2020.\",\n 'doc': 'AFWA\\'s investigation found that the woman seen in the viral photo is not Amulya Leona Noronha. Rahul Gandhi and members of the Congress party, as part of the Bharat Jodo Yatra, have met locals and leaders alike as they move from city to city on foot. Of the many, many photos from the road, one shows Gandhi posing with a young woman. The Congress leader was giving her a side-hug. This photo has been widely shared with some questionable claims. They alleged that the woman in the photo is Amulya Leona Noronha, a student who chanted “Pakistan Zindabad” during an anti-CAA rally, while sharing the stage with MP Asaduddin Owaisi in February 2020. People shared the photo with captions like, \"Remember her who said Pak Zindabad, you can see her with Pappu in Bharat Todo Yatra.\" The Archived version of one such post can be seen here . It was tweeted by Bharatiya Janata Party leader Priti Gandhi, who said, \"Look carefully. Not Bharat Jodo, this is Bharat Todo!\" She later deleted the tweet. Remember her who said PaK Zindabad, you can see her with Pappu in #BharatTodoYatra pic.twitter.com/acmCn4A4Mo — AG #MakingOfANewIndia (@AG4BJP) September 23, 2022 AFWA\\'s investigation found that the woman seen in the viral photo is not Amulya Leona Noronha. AFWA Probe We noticed tweets in responses to such tweets claiming that the woman in the photo was Miva Andreleo. We searched for Miva Andreleo and found her social media accounts. On her Instagram, she shared the same photo of herself with Rahul Gandhi. A comparison between both makes it easy to understand that both are the same. Who is Miva Andreleo? As her profile mentioned, Andreleo is a member of the Kerala Students Union (KSU). She also works for the Congress party. She also posted a video of her meeting Gandhi during the Bharat Jodo Yatra in Kerala. Therefore, it is clear that the woman seen with Rahul Gandi is not Amulya Leona Noronha, but Miva Andreleo, a student leader from Kerala. --- ENDS --- INDIA TODAY FACT CHECK Claim During the Bharat Jodo Yatra, Rahul Gandhi was seen with Amulya Leona Noronha, the woman who was arrested for saying \\'Pakistan Zindabad\\' during an anti-CAA rally in 2020. Conclusion This is not Amulya Leona. The woman seen with Rahul Gandhi is Miva Andreleo of the Kerala Students Union (KSU). JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false',\n 'taxonomy_label': 'temporal',\n 'label_original': 'False'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"model_type = \"TEMPORAL\" # \"TOP5\" \"DECOMP\" \"TEMPORAL\" \"ORACLE\"","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:11.177638Z","iopub.execute_input":"2024-06-13T12:19:11.177927Z","iopub.status.idle":"2024-06-13T12:19:11.181878Z","shell.execute_reply.started":"2024-06-13T12:19:11.177901Z","shell.execute_reply":"2024-06-13T12:19:11.180974Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if model_type == \"DECOMP\":\n    with open(\"/kaggle/input/quantemp-decomp-data/nli_test_decomposed_reranked_top1.json\", 'rb') as f:\n        test_top5 = json.load(f)\nelif model_type == \"TEMPORAL\":\n    with open(\"/kaggle/input/quantemp-temporal-rerank/nli_input_test_reranktop5_temporal.json\", 'rb') as f:\n        test_top5 = json.load(f)\nelif model_type == \"TOP5\":\n    with open(\"/kaggle/input/quantemp-evidence-snippets/test/nli_input_test_reranktop5.json\", 'rb') as f:\n        test_top5 = json.load(f)\nelif model_type == \"ORACLE\":\n    with open(\"/kaggle/input/quantemp/test_claims_quantemp.json\", 'rb') as f:\n        test_top5 = json.load(f)\n\nassert len(test_data) == len(test_top5)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:11.183041Z","iopub.execute_input":"2024-06-13T12:19:11.183340Z","iopub.status.idle":"2024-06-13T12:19:11.225203Z","shell.execute_reply.started":"2024-06-13T12:19:11.183313Z","shell.execute_reply":"2024-06-13T12:19:11.224257Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Model Loading","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AutoModel, T5ForSequenceClassification\nfrom transformers import T5ForSequenceClassification, T5Tokenizer\n\nclass MultiClassClassifier(nn.Module):\n  def __init__(self, model_path, labels_count, hidden_dim=512, mlp_dim=256, dropout=0.1, freeze_model=False):\n    super().__init__()\n\n    # automodel gives T5Model\n    self.model = AutoModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n    self.mlp = nn.Sequential(\n        nn.Linear(hidden_dim, mlp_dim),\n        nn.ReLU(),\n        nn.Linear(mlp_dim, labels_count)\n    )\n\n    if freeze_model:\n      print(\"freezing layers\")\n      for param in self.model.parameters():\n          param.requires_grad = False\n\n  def forward(self, input_ids, attention_mask):\n    model_out = self.model.encoder.forward(input_ids=input_ids, attention_mask=attention_mask)\n    hs = model_out.last_hidden_state\n    batch_size, _seq_length, hidden_size = hs.shape\n    # take the first token of the last hidden state, because it captures all the context\n    x = hs.view(batch_size, -1, hidden_size)[:, 0, :]\n    x = self.mlp(x)\n    \n    # x = self.dropout(x)\n    # x = self.mlp(x.float())\n    return x\n\nif model_type == \"DECOMP\":\n    tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/fact-checker-nt5/pytorch/claim-decomp-v2/1/model_roberta_large_oracle\")\n    model = MultiClassClassifier(\"nielsr/nt5-small-rc1\", hidden_dim=512, mlp_dim=256, labels_count=3, freeze_model=False)\n    checkpoint = torch.load(\"/kaggle/input/fact-checker-nt5/pytorch/claim-decomp-v2/1/checkpoint.pt\")\n    \nelif model_type == \"TOP5\":\n    tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/fact-checker-nt5/pytorch/rerank-top5/1/model_roberta_large_oracle\")\n    model = MultiClassClassifier(\"nielsr/nt5-small-rc1\", hidden_dim=512, mlp_dim=256, labels_count=3, freeze_model=False)\n    checkpoint = torch.load(\"/kaggle/input/fact-checker-nt5/pytorch/rerank-top5/1/checkpoint.pt\")\n    \nelif model_type == \"ORACLE\":\n    tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/fact-checker-nt5/pytorch/no-preprocess/2/nt5_trained_nocos2-20240521T084838Z-001/nt5_trained_nocos2\")\n    model = MultiClassClassifier(\"nielsr/nt5-small-rc1\", hidden_dim=512, mlp_dim=256, labels_count=3, freeze_model=False)\n    checkpoint = torch.load(\"/kaggle/input/fact-checker-nt5/pytorch/no-preprocess/2/checkpoint.pt\")\n    \nelif model_type == \"TEMPORAL\":\n    tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/fact-checker-nt5/pytorch/temporal-rerank/1/model_roberta_large_oracle\")\n    model = MultiClassClassifier(\"nielsr/nt5-small-rc1\", hidden_dim=512, mlp_dim=256, labels_count=3, freeze_model=False)\n    checkpoint = torch.load(\"/kaggle/input/fact-checker-nt5/pytorch/temporal-rerank/1/checkpoint.pt\")\n    \nmodel.load_state_dict(checkpoint)\nmodel.to(device)\nprint(\"loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:11.228748Z","iopub.execute_input":"2024-06-13T12:19:11.229128Z","iopub.status.idle":"2024-06-13T12:19:12.291471Z","shell.execute_reply.started":"2024-06-13T12:19:11.229098Z","shell.execute_reply":"2024-06-13T12:19:12.290478Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"taxonomy_labels = {}\nlabels = {}\nfor data in [test_data]:\n  for d in data:\n    d['taxonomy_label'] = d['taxonomy_label'].strip()\n    l = d['taxonomy_label']\n    if l in taxonomy_labels:\n      taxonomy_labels[l] += 1\n    else:\n      taxonomy_labels[l] = 1\n\n    l = d['label']\n    if l in labels:\n      labels[l] += 1\n    else:\n      labels[l] = 1\n\nprint(taxonomy_labels)\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:12.292810Z","iopub.execute_input":"2024-06-13T12:19:12.293670Z","iopub.status.idle":"2024-06-13T12:19:12.305161Z","shell.execute_reply.started":"2024-06-13T12:19:12.293632Z","shell.execute_reply":"2024-06-13T12:19:12.304097Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"{'statistical': 1210, 'temporal': 683, 'comparison': 255, 'interval': 347}\n{'Conflicting': 598, 'True': 474, 'False': 1423}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\nLE.fit(list(labels.keys()))\n\nLE_taxonomy = LabelEncoder()\nLE_taxonomy.fit(list(taxonomy_labels.keys()))\n\ntest_labels = torch.tensor(LE.transform([fact[\"label\"] for fact in test_data]))\ntest_taxonomy_labels = torch.tensor(LE_taxonomy.transform([fact[\"taxonomy_label\"] for fact in test_data]))\n\nprint(list(labels.keys()))\nprint(test_labels[:20])\n\nprint(list(taxonomy_labels.keys()))\nprint(test_taxonomy_labels[:20])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:12.307711Z","iopub.execute_input":"2024-06-13T12:19:12.308013Z","iopub.status.idle":"2024-06-13T12:19:12.324187Z","shell.execute_reply.started":"2024-06-13T12:19:12.307988Z","shell.execute_reply":"2024-06-13T12:19:12.323271Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['Conflicting', 'True', 'False']\ntensor([0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1])\n['statistical', 'temporal', 'comparison', 'interval']\ntensor([2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 0])\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_features_top5_decomp(data):\n  features = []\n  evidences = []\n\n  for index, fact in enumerate(data):\n    claim = fact[\"claim\"]\n\n    feature = \"[Claim]:\"+claim+\"[Questions]:\"+fact['decomposed_questions']+\"[Evidences]:\"+fact[\"evidence\"]\n    features.append(feature)\n  return features\n\ndef get_features_oracle(data):\n  features = []\n  evidences = []\n\n  for index, fact in enumerate(data):\n    claim = fact[\"claim\"]\n\n    feature = \"[Claim]:\"+claim+\"[Evidences]:\"+fact[\"doc\"]\n    features.append(feature)\n  return features\n\ndef get_features(data):\n  features = []\n  evidences = []\n\n  for index, fact in enumerate(data):\n    claim = fact[\"claim\"]\n\n    feature = \"[Claim]:\"+claim+\"[Evidences]:\"+fact[\"evidence\"]\n    features.append(feature)\n  return features\n\n\nif model_type == \"DECOMP\":\n    test_features = get_features_top5_decomp(test_top5)\nelif model_type == \"ORACLE\":\n    test_features = get_features_oracle(test_top5)\nelif model_type == \"TEMPORAL\" or model_type == \"TOP5\":\n    test_features = get_features(test_top5)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:12.325376Z","iopub.execute_input":"2024-06-13T12:19:12.325649Z","iopub.status.idle":"2024-06-13T12:19:12.338476Z","shell.execute_reply.started":"2024-06-13T12:19:12.325625Z","shell.execute_reply":"2024-06-13T12:19:12.337571Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef encode(features):\n  input_ids = []\n  attention_masks = []\n  for sent in tqdm(features):\n      # `encode_plus` will:\n      #   (1) Tokenize the sentence.\n      #   (2) Prepend the `[CLS]` token to the start.\n      #   (3) Append the `[SEP]` token to the end.\n      #   (4) Map tokens to their IDs.\n      #   (5) Pad or truncate the sentence to `max_length`\n      #   (6) Create attention masks for [PAD] tokens.\n      encoded_dict = tokenizer.encode_plus(\n                          sent,                      # Sentence to encode.\n                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                          max_length = 256,           # Pad & truncate all sentences.\n                          padding = 'max_length',\n                          truncation=True,\n                          return_attention_mask = True,   # Construct attn. masks.\n                          return_tensors = 'pt',     # Return pytorch tensors.\n                    )\n\n      # Add the encoded sentence to the list.\n      input_ids.append(encoded_dict['input_ids'])\n\n      # And its attention mask (simply differentiates padding from non-padding).\n      attention_masks.append(encoded_dict['attention_mask'])\n  # Convert the lists into tensors.\n  input_ids = torch.cat(input_ids, dim=0)\n  attention_masks = torch.cat(attention_masks, dim=0)\n  return input_ids, attention_masks\n\ntest_input_ids, test_attention_masks = encode(test_features)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', test_features[0])\nprint('Token IDs:', test_input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:12.339747Z","iopub.execute_input":"2024-06-13T12:19:12.340019Z","iopub.status.idle":"2024-06-13T12:19:20.363488Z","shell.execute_reply.started":"2024-06-13T12:19:12.339995Z","shell.execute_reply":"2024-06-13T12:19:20.362519Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 2495/2495 [00:07<00:00, 312.19it/s]","output_type":"stream"},{"name":"stdout","text":"Original:  [Claim]:\"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\"[Evidences]:this non-partisan congressional budget office report confirms that obamacare will cost america 2 million jobs ... budget analysts said in the most detailed ... ... earlier this year, the head of the congressional budget office testified before congress that implementation of obamacare would cost 800,000 jobs.  ... jun 14, 2011  ... congressional budget office testified before congress that implementation of obamacare would cost 800,000 jobs.\" bachmann made nearly the ... the director of the congressional budget office testified last year that obamacare will destroy 800,000 jobs and this summer the u.s. chamber of commerce ... 2014-02-24  the non-partisan congressional budget office recently reported that obamacare will shrink the economy by the equivalent of 2.5 million full-time ...\nToken IDs: tensor([  784,   254,   521,   603,   908,    10,   121,   634,   529,    18,\n        18237,     3, 27998, 12532,  2126, 12179,  4534,  6936,    15,    56,\n          583,     8,   412,     5,   134,     5,    72,   145,   927,   632,\n          632,     3,     6,   632,   632,   632,  2476,   535,  6306,   427,\n         6961,  1433,     7,   908,    10,  8048,   529,    18, 18237, 28167,\n         1487,   828,   934,  3606,     7,    24,     3,    32,   115,   265,\n            9,  2864,    56,   583,     3, 23064,   357,   770,  2476,     3,\n          233,  1487, 15639,   243,    16,     8,   167,  3117,     3,   233,\n            3,   233,  2283,    48,   215,     6,     8,   819,    13,     8,\n        28167,  1487,   828,   794,  3676,   274, 27197,    24,  4432,    13,\n            3,    32,   115,   265,     9,  2864,   133,   583,   927,   632,\n          632,     3,     6,   632,   632,   632,  2476,     5,     3,   233,\n            3,  6959,   536,   591,     3,     6,   357,   632,   536,   536,\n            3,   233, 28167,  1487,   828,   794,  3676,   274, 27197,    24,\n         4432,    13,     3,    32,   115,   265,     9,  2864,   133,   583,\n          927,   632,   632,     3,     6,   632,   632,   632,  2476,   535,\n            3,  6425,  2434,   263,  2111,     8,     3,   233,     8,  2090,\n           13,     8, 28167,  1487,   828,   794,  3676,   336,   215,    24,\n            3,    32,   115,   265,     9,  2864,    56, 10123,   927,   632,\n          632,     3,     6,   632,   632,   632,  2476,    11,    48,  1248,\n            8,     3,    76,     5,     7,     5, 10751,    13, 12794,     3,\n          233,   357,   632,   536,   591,     3,    18,   632,   357,     3,\n           18,   357,   591,     8,   529,    18, 18237, 28167,  1487,   828,\n         1310,  2196,    24,     3,    32,   115,   265,     9,  2864,    56,\n        18508,     8,  2717,    57,     8,  7072,    13,   357,     3,     5,\n          755,   770,   423,    18,   715,     1])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# create DataLoaders\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels, test_taxonomy_labels)\n\nBATCH_SIZE = 16\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset),\n            batch_size = BATCH_SIZE\n        )","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:20.365151Z","iopub.execute_input":"2024-06-13T12:19:20.365492Z","iopub.status.idle":"2024-06-13T12:19:20.372992Z","shell.execute_reply.started":"2024-06-13T12:19:20.365463Z","shell.execute_reply":"2024-06-13T12:19:20.372022Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# ========================================\n#               Testing\n# ========================================\nfrom collections import Counter\nimport time\nimport datetime\nfrom torch.optim import AdamW\nfrom torch import nn\nimport numpy as np\n\nloss_func = nn.CrossEntropyLoss()\n\ndef test(model, dataloader):\n    print(\"\")\n    print(\"Running Test set...\")\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    correct_counts = Counter()\n    total_counts = Counter()\n    \n    results = {}\n\n    # Evaluate data for one epoch\n    for batch in tqdm(dataloader):\n\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using\n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        b_taxonomy_labels = batch[3]\n\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():\n\n            # Forward pass, calculate logit predictions.\n            logits = model(b_input_ids,b_input_mask)\n\n        # Accumulate the validation loss.\n        loss = loss_func(logits, b_labels)\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        \n        # Populate the dictionary\n        predictions = np.argmax(logits, axis=1)\n        categories = LE_taxonomy.inverse_transform(b_taxonomy_labels)\n        for pred, label, category in zip(predictions, label_ids, categories):   \n            if category not in results:\n                results[category] = {'predictions': [], 'labels': []}\n            results[category]['predictions'].append(pred)\n            results[category]['labels'].append(label)\n\n\n    for category in results:\n        for nested in results[category]:\n            results[category][nested] = np.array(results[category][nested])\n\n    return results\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:22:29.514390Z","iopub.execute_input":"2024-06-13T12:22:29.514759Z","iopub.status.idle":"2024-06-13T12:22:29.527672Z","shell.execute_reply.started":"2024-06-13T12:22:29.514730Z","shell.execute_reply":"2024-06-13T12:22:29.526616Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"res = test(model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:22:33.129189Z","iopub.execute_input":"2024-06-13T12:22:33.129862Z","iopub.status.idle":"2024-06-13T12:22:38.701257Z","shell.execute_reply.started":"2024-06-13T12:22:33.129832Z","shell.execute_reply":"2024-06-13T12:22:38.699991Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nRunning Test set...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:05<00:00, 28.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# average = macro/weighted/micro\ndef compute_metrics(pred_flat, labels_flat):\n    accuracy = np.sum(pred_flat.flatten() == labels_flat.flatten()) / len(labels_flat.flatten())\n    \n    p_m = precision_score(labels_flat, pred_flat, average='macro')\n    p_w = precision_score(labels_flat, pred_flat, average='weighted')\n    r_m = recall_score(labels_flat, pred_flat, average='macro')\n    r_w = recall_score(labels_flat, pred_flat, average='weighted')\n\n    \n    f1_m = f1_score(labels_flat, pred_flat, average='macro')\n    f1_w = f1_score(labels_flat, pred_flat, average='weighted')\n    \n    return accuracy, p_m, p_w, r_m, r_w, f1_m, f1_w","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:20.505068Z","iopub.status.idle":"2024-06-13T12:19:20.505404Z","shell.execute_reply.started":"2024-06-13T12:19:20.505242Z","shell.execute_reply":"2024-06-13T12:19:20.505256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_preds = np.array([])\ntotal_labels = np.array([])\n\nprint(f\"# {model_type}\")\n\n# per taxonomy label\nfor category in res.keys():\n    pred = res[category]['predictions']\n    labels = res[category]['labels']\n    acc, p_m, p_w, r_m, r_w, f1_m, f1_w = compute_metrics(pred, labels)\n    \n    print(\"\")\n    print(f\"\"\"[{category}]:\n    M-P : {p_m*100:.2f}\n    W-P : {p_w*100:.2f}\n    M-R : {r_m*100:.2f}\n    W-R : {r_w*100:.2f}\n    M-F1: {f1_m*100:.2f}\n    W-F1: {f1_w*100:.2f}\"\"\")\n    \n    total_preds = np.concatenate((total_preds, pred))\n    total_labels = np.concatenate((total_labels, labels))\n\n\nscores = f1_score(total_labels, total_preds, average=None)\nfor i, score in enumerate(scores):\n    l = LE.inverse_transform([i])[0]\n    print(\"\")\n    print(f\"\"\"[{l}]:\n    F1: {score*100:.2f}\"\"\")\n\nacc, p_m, p_w, r_m, r_w, f1_m, f1_w = compute_metrics(total_preds, total_labels)\nprint(f\"\"\"\n\nTotal:\n    M-P : {p_m*100:.2f}\n    W-P : {p_w*100:.2f}\n    M-R : {r_m*100:.2f}\n    W-R : {r_w*100:.2f}\n    M-F1: {f1_m*100:.2f}\n    W-F1: {f1_w*100:.2f}\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:19:20.506863Z","iopub.status.idle":"2024-06-13T12:19:20.507179Z","shell.execute_reply.started":"2024-06-13T12:19:20.507021Z","shell.execute_reply":"2024-06-13T12:19:20.507034Z"},"trusted":true},"execution_count":null,"outputs":[]}]}