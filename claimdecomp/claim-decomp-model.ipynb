{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8584396,"sourceType":"datasetVersion","datasetId":5134154}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport nltk \nimport os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nimport torch\nnltk.download('punkt')\n\n\nFILES = [\"/kaggle/input/claimdecomp/train.jsonl\", \"/kaggle/input/claimdecomp/dev.jsonl\", \"/kaggle/input/claimdecomp/test.jsonl\"] \n\ndata = []\nfor file in FILES:\n    with open(file, 'rb') as f:\n        data.append([{\"claim\": json.loads(d)['claim'], \"questions\": \" \".join(json.loads(d)['annotations'][0]['questions'])} for d in f.readlines()])\ntrain_data, val_data, test_data = data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-02T13:08:47.853615Z","iopub.execute_input":"2024-06-02T13:08:47.853951Z","iopub.status.idle":"2024-06-02T13:08:54.273011Z","shell.execute_reply.started":"2024-06-02T13:08:47.853924Z","shell.execute_reply":"2024-06-02T13:08:54.271937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:08:58.094992Z","iopub.execute_input":"2024-06-02T13:08:58.095507Z","iopub.status.idle":"2024-06-02T13:09:15.184294Z","shell.execute_reply.started":"2024-06-02T13:08:58.095460Z","shell.execute_reply":"2024-06-02T13:09:15.183137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If there's a GPU available...\nif torch.cuda.is_available():\n\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:15.186341Z","iopub.execute_input":"2024-06-02T13:09:15.186714Z","iopub.status.idle":"2024-06-02T13:09:15.280859Z","shell.execute_reply.started":"2024-06-02T13:09:15.186680Z","shell.execute_reply":"2024-06-02T13:09:15.279818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:15.281917Z","iopub.execute_input":"2024-06-02T13:09:15.282221Z","iopub.status.idle":"2024-06-02T13:09:15.287216Z","shell.execute_reply.started":"2024-06-02T13:09:15.282196Z","shell.execute_reply":"2024-06-02T13:09:15.286201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer\n\n# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n# example_english_phrase = \"Unemployment is low because everyone has two jobs. Unemployment is low because people are working 60, 70, 80 hours a week and can barely feed their family.\"\n# batch = tokenizer(example_english_phrase, return_tensors=\"pt\")\n# generated_ids = model.generate(batch[\"input_ids\"])\n# print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:23.494099Z","iopub.execute_input":"2024-06-02T13:09:23.494669Z","iopub.status.idle":"2024-06-02T13:09:23.720453Z","shell.execute_reply.started":"2024-06-02T13:09:23.494634Z","shell.execute_reply":"2024-06-02T13:09:23.719439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\nmax_input_length = 64\nmax_target_length = 128\n\ndef preprocess_function(examples, prefix=\"decompose the claim into subquestions:\"):\n    claims = [d['claim'] for d in examples]\n    questions = [d['questions'] for d in examples]\n    inputs = [prefix + doc for doc in claims]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # The \"labels\" are the tokenized outputs:\n    labels = tokenizer(text_target=questions, max_length=max_target_length, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:26.947441Z","iopub.execute_input":"2024-06-02T13:09:26.947852Z","iopub.status.idle":"2024-06-02T13:09:28.481865Z","shell.execute_reply.started":"2024-06-02T13:09:26.947823Z","shell.execute_reply":"2024-06-02T13:09:28.480928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create DataLoaders\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, dic):\n        \"\"\"\n        Args:\n            tensor_dataset (TensorDataset): A TensorDataset object containing your data.\n        \"\"\"\n        self.dic = dic\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.dic['input_ids'])\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a sample from the dataset at the given index.\n        \n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: A tuple containing the data and target for the given index.\n        \"\"\"\n        return {'input_ids': self.dic['input_ids'][idx], 'labels': self.dic['labels'][idx], 'attention_mask': self.dic['attention_mask'][idx]}\n    \n\ntrain_dataset = CustomDataset(preprocess_function(train_data))\nval_dataset = CustomDataset(preprocess_function(val_data))\npreprocess_function(val_data).keys()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:33.922903Z","iopub.execute_input":"2024-06-02T13:09:33.923275Z","iopub.status.idle":"2024-06-02T13:09:34.929491Z","shell.execute_reply.started":"2024-06-02T13:09:33.923248Z","shell.execute_reply":"2024-06-02T13:09:34.928532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:36.987544Z","iopub.execute_input":"2024-06-02T13:09:36.987918Z","iopub.status.idle":"2024-06-02T13:09:36.993756Z","shell.execute_reply.started":"2024-06-02T13:09:36.987888Z","shell.execute_reply":"2024-06-02T13:09:36.992736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\")\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:09:41.825162Z","iopub.execute_input":"2024-06-02T13:09:41.825553Z","iopub.status.idle":"2024-06-02T13:10:02.951313Z","shell.execute_reply.started":"2024-06-02T13:09:41.825521Z","shell.execute_reply":"2024-06-02T13:10:02.950270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 6\nbatch_size = 8\nlr = 2e-5\n\nSAVE_PATH = \"bart/\"\nLOGGING_PATH = \"bart-logs/\"\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=SAVE_PATH,\n    learning_rate=lr,\n    do_train = True,\n    do_eval = True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    num_train_epochs=epochs,\n    predict_with_generate=True,\n    generation_max_length=512,\n    logging_dir=LOGGING_PATH,\n    logging_steps=300,\n    save_steps=300,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:10:40.114821Z","iopub.execute_input":"2024-06-02T13:10:40.115565Z","iopub.status.idle":"2024-06-02T13:10:40.158188Z","shell.execute_reply.started":"2024-06-02T13:10:40.115524Z","shell.execute_reply":"2024-06-02T13:10:40.157143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:10:42.892972Z","iopub.execute_input":"2024-06-02T13:10:42.893361Z","iopub.status.idle":"2024-06-02T13:10:42.898857Z","shell.execute_reply.started":"2024-06-02T13:10:42.893330Z","shell.execute_reply":"2024-06-02T13:10:42.897495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nmetric = datasets.load_metric(\"rouge\", trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    print(predictions[0])\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    print(\"decoded gen\",decoded_preds)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:11:52.247151Z","iopub.execute_input":"2024-06-02T13:11:52.247550Z","iopub.status.idle":"2024-06-02T13:11:52.440538Z","shell.execute_reply.started":"2024-06-02T13:11:52.247518Z","shell.execute_reply":"2024-06-02T13:11:52.439476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:11:55.997247Z","iopub.execute_input":"2024-06-02T13:11:55.997643Z","iopub.status.idle":"2024-06-02T13:11:57.432053Z","shell.execute_reply.started":"2024-06-02T13:11:55.997611Z","shell.execute_reply":"2024-06-02T13:11:57.431284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:12:02.158299Z","iopub.execute_input":"2024-06-02T13:12:02.159153Z","iopub.status.idle":"2024-06-02T13:12:02.165366Z","shell.execute_reply.started":"2024-06-02T13:12:02.159117Z","shell.execute_reply":"2024-06-02T13:12:02.164451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:12:05.302991Z","iopub.execute_input":"2024-06-02T13:12:05.303347Z","iopub.status.idle":"2024-06-02T13:13:21.350664Z","shell.execute_reply.started":"2024-06-02T13:12:05.303321Z","shell.execute_reply":"2024-06-02T13:13:21.349042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:13:28.891444Z","iopub.execute_input":"2024-06-02T13:13:28.892400Z","iopub.status.idle":"2024-06-02T13:18:11.605216Z","shell.execute_reply.started":"2024-06-02T13:13:28.892356Z","shell.execute_reply":"2024-06-02T13:18:11.603891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:20:48.232433Z","iopub.execute_input":"2024-06-02T13:20:48.232918Z","iopub.status.idle":"2024-06-02T13:21:25.761757Z","shell.execute_reply.started":"2024-06-02T13:20:48.232872Z","shell.execute_reply":"2024-06-02T13:21:25.760294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('bart-model/')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:24:29.837443Z","iopub.execute_input":"2024-06-02T13:24:29.838349Z","iopub.status.idle":"2024-06-02T13:24:32.804445Z","shell.execute_reply.started":"2024-06-02T13:24:29.838314Z","shell.execute_reply":"2024-06-02T13:24:32.803381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r model.zip /kaggle/working/bart-model","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:27:58.646382Z","iopub.execute_input":"2024-06-02T13:27:58.647897Z","iopub.status.idle":"2024-06-02T13:29:30.669646Z","shell.execute_reply.started":"2024-06-02T13:27:58.647853Z","shell.execute_reply":"2024-06-02T13:29:30.668416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def generate_output(test_samples, model):\n    inputs = tokenizer(\n        test_samples,\n        max_length=128,\n        return_tensors=\"pt\")\n\n    input_ids = inputs.input_ids.to(model.device)\n    print(input_ids)\n    print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask,min_length = 64, max_length = 128, do_sample=True, top_p=0.95, top_k=50)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return output_str","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:21:45.493483Z","iopub.execute_input":"2024-06-02T13:21:45.494254Z","iopub.status.idle":"2024-06-02T13:21:45.501214Z","shell.execute_reply.started":"2024-06-02T13:21:45.494222Z","shell.execute_reply":"2024-06-02T13:21:45.500156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_output(\"decompose the claim into subquestions:FAKE:  Commandos from Berkut who refused to kneel have been burned alive in Lviv.\", model)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T13:21:47.486593Z","iopub.execute_input":"2024-06-02T13:21:47.487204Z","iopub.status.idle":"2024-06-02T13:21:48.848609Z","shell.execute_reply.started":"2024-06-02T13:21:47.487173Z","shell.execute_reply":"2024-06-02T13:21:48.847632Z"},"trusted":true},"execution_count":null,"outputs":[]}]}